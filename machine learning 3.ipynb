{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c95d5c6-f96a-4f5d-87b5-24b6e77a3604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 1 \n",
    "# \"Simple linear regression\"\n",
    "# It is comes under supervised machine learning \n",
    "# Simple linear regression involves predicting a dependent variable based on a single independent variable\n",
    "# In this regression model output feature or dependent feature is continous \n",
    "# It consists of only one independent feature\n",
    "# for example: \n",
    "# A Dataset consisting of weight and height feature \n",
    "# where weight is independent feature and height is dependent feature\n",
    "# model should predict the height when we give weight \n",
    "\n",
    "# \"Multiple linear regression\" \n",
    "# It is comes under supervised machine learning\n",
    "# In this regression model output feature or dependent feature is continous\n",
    "# Multiple linear regression extends the concept of simple linear regression by \n",
    "#   incorporating multiple independent variables to predict the dependent variable.\n",
    "# It consists of more one independent features but only dependent feature\n",
    "# for example house price prediction \n",
    "# features like no.of rooms , location are independent features and price will be the dependent feature\n",
    "# model should predict the price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fa6ead1-d45d-49f5-91f4-8220dcb3698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 2 :\n",
    "# Assumptions \n",
    "# 1. Linearity: This means that the change in the dependent variable is proportional to the change in the independent variables.\n",
    "# 2. Independence: This assumption ensures that the observations do not influence each other's values.\n",
    "# 3. Normality: The error terms or residuals should follow a normal distribution \n",
    "\n",
    "# we can check it by following techniques:\n",
    "# 1.Residual analysis: Plotting the residuals against the predicted values can help assess linearity\n",
    "#     If the residuals show a random pattern around zero and have a consistent spread, the assumptions are likely met.\n",
    "# 2.Normality tests: Conducting tests like the Shapiro-Wilk test, Anderson-Darling test, \n",
    "#    or visual inspection of a histogram or Q-Q plot of the residuals can indicate whether they follow a normal distribution.\n",
    "# 3.Outlier analysis: Identifying outliers in the dataset can affect the linearity and normality assumptions. \n",
    "#     Outliers can be detected through methods like box plots, scatter plots, or leverage plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06b0ace5-1667-4b69-9d8f-f4f2410920ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 3 :\n",
    "# Intercept can be interpreted by , where the best fit line meets the y-axis\n",
    "# The intercept represents the estimated value of the dependent variable when all independent variables are set to zero.\n",
    "# It indicates the starting point or baseline value of the dependent variable.\n",
    "# for example \n",
    "# a salary of 25000 for a  fresher having zero years expierence \n",
    "\n",
    "# slope can be interpreted by , when unit movement in x-axis so, how much movement in y-axis\n",
    "# The slope represents the change in the dependent variable for a one-unit change in the corresponding independent variable\n",
    "# for example :\n",
    "# let's consider a linear regression model that predicts a person's salary (dependent variable) based on their years of experience (independent variable). \n",
    "# If the slope is estimated as 2000, it means that, on average, for every additional year of experience, \n",
    "# the person's salary is expected to increase by 2000, assuming all other factors remain constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39913fc6-84c1-48d7-9506-d3f30e233895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 4 :\n",
    "# Gradient descent \n",
    "# Gradient descent is an iterative optimization algorithm used in machine learning to minimize the error or cost function of a model. \n",
    "\n",
    "# It is used in machine learning alogorithm :\n",
    "# It plays a crucial role in training models by iteratively updating the model's parameters to minimize the cost function and \n",
    "# improve the model's performance.\n",
    "# 1.Model Training:\n",
    "# 2.convergence and optimization\n",
    "# 3.Different Variants of Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ea19820-c853-478c-99b7-28e68f7a9b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 5 :\n",
    "# \"Multiple linear regression\" \n",
    "# It is comes under supervised machine learning\n",
    "# In this regression model output feature or dependent feature is continous\n",
    "# Multiple linear regression extends the concept of simple linear regression by \n",
    "#   incorporating multiple independent variables to predict the dependent variable.\n",
    "# It consists of more one independent features but only dependent feature\n",
    "# for example house price prediction \n",
    "# features like no.of rooms , location are independent features and price will be the dependent feature\n",
    "# model should predict the price \n",
    "\n",
    "# It differs from simple linear regression model \n",
    "# Simple linear regression involves predicting a dependent variable based on a single independent variable\n",
    "# In this regression model output feature or dependent feature is continous \n",
    "# It consists of only one independent feature\n",
    "# for example: \n",
    "# A Dataset consisting of weight and height feature \n",
    "# where weight is independent feature and height is dependent feature\n",
    "# model should predict the height when we give weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad3dc7dc-a876-45cf-9a0e-8c08d03e7662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 6 :\n",
    "# Multicollinearity\n",
    "# Multicollinearity refers to a situation in multiple linear regression where there is a high correlation between two or more independent variables. \n",
    "# It can create problems in the regression model, making it challenging to determine the individual effects of each independent variable on the dependent variable. \n",
    "\n",
    "# we detect it by:\n",
    "# 1 .Correlation Matrix: Calculate the correlation coefficients between pairs of independent variables. \n",
    "# High correlation coefficients (close to +1 or -1) indicate potential multicollinearity.\n",
    "\n",
    "# we address it by :\n",
    "# Addressing Multicollinearity:\n",
    "# 1.Remove Redundant Variables: If two or more independent variables are highly correlated, \n",
    "#   consider removing one of them from the regression model. By eliminating redundant variables, you can reduce multicollinearity.\n",
    "# 2.Feature Selection Techniques: Use feature selection methods like backward elimination or \n",
    "#   stepwise regression to automatically identify and remove independent variables with high collinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45935102-e4e1-473a-b99e-0d45790cae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 7 :\n",
    "# Polynomial regression\n",
    "# Polynomial regression is a form of regression analysis that models the relationship between the independent variable(s) \n",
    "# and the dependent variable using polynomial functions\n",
    "# polynomial regression can capture more complex patterns and curves.\n",
    "# The degree of the polynomial determines the flexibility and complexity of the curve the model can fit.\n",
    "\n",
    "# It differs from linear model\n",
    "# 1.In contrast to linear regression, where the relationship is assumed to be a straight line, but in polynomial it is a curve \n",
    "# 2.Complexity and Flexibility: Linear regression is a simpler model that assumes a linear relationship. It is suitable when the data follows a linear trend. \n",
    "#   Polynomial regression, on the other hand, can capture more intricate relationships with curves and bends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4645574a-7d50-495e-89b1-ab67995875d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 8 :\n",
    "# Advantages\n",
    "# 1.Capturing Nonlinear Relationships: Polynomial regression can model nonlinear relationships between variables by incorporating polynomial terms. \n",
    "#   It can capture curved or nonlinear patterns that linear regression cannot handle.\n",
    "# 2.Flexibility: The degree of the polynomial can be adjusted to increase or decrease the model's complexity and flexibility. \n",
    "#   This allows for a better fit to the data when there are complex relationships or nonlinearity present.\n",
    "\n",
    "# Disadvantages\n",
    "# 1.Overfitting: Polynomial regression models with high degrees are prone to overfitting, especially when the degree is chosen excessively. \n",
    "#  Overfitting occurs when the model fits the noise or random fluctuations in the training data, leading to poor generalization on unseen data.\n",
    "# 2.Interpretability: The interpretation of the coefficients in polynomial regression becomes more complex compared to linear regression. \n",
    "#    Higher-degree polynomial terms can be challenging to interpret and explain.\n",
    "\n",
    "# It is important in situations like\n",
    "# 1. When the relationship between the variables is expected to be nonlinear or when there is evidence of nonlinear patterns in the data, polynomial regression is preferred.\n",
    "#   It can capture curves, bends, and other complex shapes.\n",
    "# 2.when data is complex, we use plynomial regression \n",
    "#     Polynomial regression provides more flexibility in modeling the data compared to linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd1db60-c06a-410c-bcd0-278e331f3c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
